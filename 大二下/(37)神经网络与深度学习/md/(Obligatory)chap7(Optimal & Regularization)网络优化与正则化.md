# **Optimization & Regularization**

主要存在的问题:优化问题(经验风险最小),泛化问题(正则化问题)

鞍点:取得的真正的全局最小值的概率为$(\frac{1}{2})^{Dimension} $

平坦最小值的好处:性能接近

优化地形:使用残差连接对于DenseNet121直接优化成接近凸函数

非凸优化问题和梯度问题

**神经网络优化的改善方法**

>   1.使用更有效的优化算法:动态学习率,梯度估计修正
>
>   2.使用更好的参数初始化方法,数据预处理方法
>
>   3.修改网络结果使得获得平坦极小值
>
>   >   优化地形,使用更好的激活函数,残差连接,逐层归一化(LN按照通道的视角归一化)
>
>   4.使用更好的超参数优化方式

**优化算法:**

>   **随机梯度下降算法(随机的取一个样本计算梯度进行迭代,但是还是需要N次)**
>
>   **Mini-Batch:计算批次的每个样本的梯度,然后求均值**
>
>   >   批量越大,随机梯度的方差越小,噪声越少,学习率可以适当上调
>   >
>   >   批量较小,需要较小的学习率,不然模型会不收敛
>
>   ==都存在随机的因素,梯度会产生扰动;并且迭代的时候学习率应该下降==
>
>   **梯度修正法**

**主要讲了AdaGrad的学习率下降+基于其的梯度的计算 = Adam优化**

**Adam = 动量法+RMSprop**

**学习率下降:逆时衰减/指数衰减/自然指数衰减/周期性学习率(三角循环,热重启)**

>   使用分段函数学习率可能会陷入尖锐极小值
>
>   使用周期变换学习率可以跳出尖锐极小值

Oth:不要下降学习率,增加Batchsize->带有warmup的大批量训练

**自适应学习率:Adagrad,RMSprop,Adadelta**

**梯度修正:动量法(带有权重t-1的添加)**

最好的是Adadelta->Adagrad->RMSprop

**梯度截断**

****

参数初始化/数据预处理

**预训练初始化(使用已知的大模型的权重进行初始化)**

**随机初始化(高斯分布,均匀分布)**

**固定值初始化(偏置为0)**

**注意参数不能全部初始化为0,神经元没有差别直接出现模式崩溃无法学习**



数据预处理

**数据的尺度不变性:全都都进行映射以后不影响学习和预测(所以可以对感兴趣的部分进行尺度的扩大->归一化(最值归一化,标准化,PCA主成分分析降维))**

****

Layer Normalization