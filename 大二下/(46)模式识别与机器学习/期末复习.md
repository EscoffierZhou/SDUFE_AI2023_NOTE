## **1.试题划分**

填空题:2分一个(40分) (主要部分,至少拿25)

选择题:5个10分(较难)

判断题:1个1分总共10分(可能全对全错)

简答题:30分(5个6分)

分析题(简答题):10分==(多层神经网络的过程,p159 8-2+公式)==

****



## **2.考点**

###### **1.简介**

p1.什么是模式

p2.什么是近邻法

p4.主动学习

p8.异常检测(除了概念以外+分类器构建正常类和异常类->正常类远远少于异常的->类别不平衡问题)

>如何定义正常和异常/定义损失函数的问题

p8.时间序列对异常检测的影响

p10.多视图学习的的主旨

###### **2.Bayes**

p16.贝叶斯公式和贝叶斯决策的定义,适用的任务

p16 贝叶斯参数估计的类别:最大似然估计...

p17

p18.贝叶斯决策的定义,贝叶斯决策的类别

>   p18.最小错误率的定义
>
>   p21.最小风险bys的定义以及目标函数公式

p22.最小风险bys的目标

p23.最小风险和最小错误率的关系:0-1损失时相同

p24.什么是判别函数

p24.构建分类器的三种思想

p25.NB和一般分类器的区别

2.6 最大似然估计/最大后验估计/EM最大化的基本思想和流程需要了解

p38.课后习题(2)

###### **3.最小二乘法和逻辑回归**

p41.什么是线性回归

p42.常用的基函数有哪些

p43.训练线性回归模型的方法:分类问题+回归问题+两者区别

p44.什么是最小二乘法+基本思想

p47.正则化和一般的最小二乘的区别(优化目标具体公式)

p48.两种正则化方法,还有\beta公式的具体表示

p50.贝叶斯线性回归的定义

p53.逻辑回归定义+表达式+通过"逻辑函数"+"回归模型"解决的是二分类问题

>   叫做"回归"的原因,sigmoid的形状

p54.逻辑回归的优化目标

p55.多类逻辑回归使用的softmax函数的基本定义

p60.思考题(4):课本的解释,不用展开太多

###### **4.SVM(重要)**

p135.什么是SVM,基于什么原理,其原理的表达

p136.大间隔原理的全部

p137.7.2 SVM的基本分类模型的全部

p138 Lagrange对偶优化:适用的问题+基本思想即可(不用写公式,但是不排除写公式!)

p140 处理线性不可分的数据,需要松弛,铰链损失的公式!!

p141 核方法公式基本思想

p143 如何检验一个函数是核函数,常见的核函数有哪些?(注意区别基函数)

p146 习题(2)

###### **5.NN**

p148.

p149.什么是感知机,如何处理数据(经过权重后输出,看作FNN)

p150.上方+只需要更新梯度,不需要反向传播

p151.构建基础是神经元,并且神经元有激活函数(而不是符号函数)+常用的激活函数的表达公式+特点(可导->导数表示)

p153.all

p155.8.2.3反向传播算法(p156~p158,基本思想,==10分=]=)

p159.NN和浅层网络的区别+深层出现的问题

p160.什么是过拟合问题+产生原因+解决方法

p163.什么是局部极值问题+产生原因+解决方法

p163.什么是梯度消失问题+产生原因+解决方法

p166.常用的NN(p166的(1)~(6),Transformer的构成+注意力机制),都要了解

p177 Transformer+注意力+自注意力的作用

p180 1/2/3

###### **6.聚类**

p205 监督和非监督的区别+聚类的作用(这里也可以异常检测)+聚类的分类(两种非常重要)+聚类算法的分类(三类,前两个是数据相似度+1密度估计)

p206