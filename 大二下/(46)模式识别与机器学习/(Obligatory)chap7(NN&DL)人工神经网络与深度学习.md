# **人工神经网络与深度学习**

MLP&algorithm,Multi-LayerNN & BP

## **1.Perceptron**

**感知器是一种二分类线性分类器**
$$
f(x)= sign(w^T x +b) = \begin{cases}
+1 & w^T x+b \geq 0
\\
\\
-1 & Oth
\end{cases}
$$
**<font color=red>其分类思路和SVM相似(错分类样本到超平面的距离)</font>**
$$
-\frac{1}{||w||}\sum_{x_i\in M}y_i(w^T x_i+b)
\\
\\
\therefore argminL(w,b) = -\sum_{x_i\in M}y_i(w^T x_i+b)
$$
**->梯度更新**
$$
\frac{\partial L_i(w,b)}{\partial w} = -y_i x_i\qquad \frac{\partial L_i(w,b)}{\partial b} = -y_i
$$

## **2.Multi-Layer NN**

###### **1.神经元**

**神经元的输入输出表达式**
$$
h(x) = f(\sum^{D}_{d=1}w_dx_d+b) = f(w^Tx)
$$

###### **2.激活函数f()**

$$
\begin{align}
f(x) = \frac{1}{1+e^{-x} } \qquad &  f'(x)=


\end{align}
$$



## **3.Deep NN**

#### **1.浅层和深层神经网络**

###### **1.基本思想**

网路层数的增加需要设计更好的网络结构和更加稳定的优化算法

**浅层和深层不是相对的意思,而是前者只有一个隐藏层(但是实际还是上下文都是相对的)**

万能逼近:对任意的连续函数,只要这个单隐藏层有充分多的神经元,都可以由包含单层的NN实现

###### **2.二级结论**

**对于thin+deep和fat+shllow的NN而言,窄(单层神经元少)而深的网络性能更好**

>   **总参数更少,数据需求量更少**

###### **3.线性代数补充($A^{100} $)**

**使用SVD分解:对角矩阵的100次方就是$U \delta^{100} U^T $**

**出现梯度爆炸和梯度消失的问题**

###### **4.深度学习和深度网络的区别**

**深度神经网络是深度学习的重要组成部分,但是深度学习中存在多种模型**

**==所以深度神经网络本质是MLP,但是深度学习网络的本质是不同的,比如Transformer==**

## ->深度增加导致的问题

#### **2.过拟合问题**

(模型泛化能力下降)

主要起因:模型数据太少,然而模型结构复杂

解决方法:

早停法:在训练Acc和测试Acc中做一个权衡

权重衰减(weight decay):对W进行正则化防止权重参数过大

丢弃法(Dropoout):神经网络关闭部分神经元(使得上一层传入的权重等于0)

解决思路:降低模型能力

#### **3.局部极值问题**

(越复杂局部最优解越多)

解决方法:

随机梯度下降:多种梯度的联合(单时间多层联动)

带有动量的梯度下降(前后时间单层联动,上一次计算的结果作为一个小的偏置项(惯性))

多次随机初始化

#### **4.梯度消失问题**

(模型层数增多,计算反向传播得到的梯度的幅度值会急剧增大或减少)

解决方法:

逐层与训练结合微调

>   对于前m层,训练几轮稳定后,再加一层,再训练稳定...再对所有n层一起更新
>
>   最后的全部更新很快,也叫微调(AlexNEt)

使用合适的激活函数(避免神经元死亡/可学习的参数ReLU)

使用设计特殊的网络结构

## **4.Normal NN**

###### **1.自编码网络**

**和MLP结构一样,但是思路是对x先编码再解码**

**玻尔兹曼机:神经元之间都有连接(玻尔兹曼机/受限玻尔兹曼机/深度玻尔兹曼机)**

>   **主要应用:学习数据间的固有内在表示,神经元节点的状态是0或1**

###### **2.CNN**

**提取特征的目标:某种模式总是存在于局部区域,相同的模式会出现多个区域,下采样不影响识别**

此时使用CNN

Padding的使用其实是图像分割中给前景增加背景,给分类器,判断是前景还是背景



###### **3.RNN**

主要目的是编码时序



LSTM:避免记忆容量爆掉,并且避免遗忘,其中的三种W和b会自行学习出来
$$
c_t= f_t  {t-1}+i\cdot tanh(W_c[x_t,h_{t-1}]^T)+c
$$


###### **4.Transformer**

注意力机制:输入序列和输出序列的关系

自注意力机制:捕获文本序列内部的依赖关系

## **EXperiment**

VGG-Net(Conv+Maxpool+FC+Softmax)

>Award winning ConvNets from 2014 ImageNet

ResNet(Conv+Residual Connection)

>Deep residual networks pre-trained on ImageNet





1.可导的损失函数->近似计算避免计算每层的梯度

2.什么是多层NN?如何训练NN?(重点)





##  **1.MLP**

###### **1.Neoron**

**1.神经元:对上一层的所有神经元输出的加权和**

**2.目标:拟合一个复杂函数**

**3.==训练的时候使用误差反向传播算法,NN的核心功能是<font color=red>有监督学习场景下的函数</font>==**

###### **2.Perceptron**

$$
f(x) = sign(wx+b) = 
\begin{cases}
+1& wx+b >0\\\\
-1 & otherwise
\end{cases}
$$

**学习的参数是权重w和偏置项b**

>   这里的sign不是非线性因素而是判断函数
>
>   并且机器学习的w是矩阵,此处的w是vec(对应内积操作)
>
>   (并且最后要多一个D+1维用于存储偏置)
>
>   对于SVM使用的基于最大间隔原理进行优化.NN使用SGD下降

###### **3.SGD**

首先进行随机非0初始化

对于单个样本$(x_i,y_i)$和损失函数$L(x,y) $关于模型参数w,b的梯度等于

更新的方向就是负梯度方向,然后根据学习率作为步长进行更新

>   这里的stochastic指的是对于大样本数据计算随机样本子集的梯度==(存在误差?)==

(下面是对SVM的SGD)
$$
-\frac{1}{||w||}\sum_{x_i\in m}y_i(x_i+b) = argmin
\sum_{x_i\in m}y_i(x_i+b)(Loss Function)
$$

$$
\frac{\partial L_i(w,b)}{\partial w} =  -y_ix_i\\
\\
\frac{\partial L_i(w,b)}{\partial b} =  -y_i\\
$$

###### **4.Multi-Layer NN 神经元**

本质上是将Perceptron的符号判断函数换成非线性激活函数

(一种特殊的Perceptron)

###### **5.Activation Function**

略



## **2.Multi-Layer NN & BP**

**多层神经网络特指只有一个隐藏层的神经网络**

机器学习从0开始

对于输入层隐藏层输出层的结构(012)结果是2层(只算隐藏层和输出层)

**==这部分看书公式==**





## **3.拓展**

CNN的核心操作是卷积操作,MLP的核心操作是加权求和,NN的核心也是加权求和

>   CNN的卷积此操作其实也是窗口移动中的加权求和

CNN核大小一般是3/7,很少是9/11,但是研究表示31(不计代价)的性能最好

CNN核的正负可以提取不同特征,如果存在可正可负,可以识别边缘

```
w_1=1 w_2=-1
 0 -3 0 5 0  -> 突变检测边缘
2 2  5 5 0 0 
```

## **4.RNN&LSTM**

RNN的长距离依赖问题

LSTM中引入了遗忘门,输出门,输入门->在输入x以后得到3个向量

>   对遗忘门和输入门进行线性组合->c
>   $$
>   c_t = f_t \circ c_{t-1}+i \circ tanh(W_t[x^*_t,h_{t-1}]+b_i)
>   $$
>
>   >   使用tanh的原因避免新信号过强减少冲击
>
>   对c和输出门进行点乘
>   $$
>   h_t= o_t\circ tanh(c_t)
>   $$

多头注意力的来源于数据库

>   并不是加速,而是增加灵活性->解码速度更快

mamba的性能比Transformer好?!

XLSTM的优化可以研究(跟牢登)

